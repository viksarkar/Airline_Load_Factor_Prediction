{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('T100_2019.csv')\n",
    "data = data.dropna(subset=['UNIQUE_CARRIER_NAME', 'ORIGIN', 'DEST'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['LOAD_FACTOR'] = data['PASSENGERS']/data['SEATS']\n",
    "data[data['LOAD_FACTOR']>1]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduledflights = data[(data['CLASS'].isin(['A','C','E','F'])) & (data['SEATS']>0)] #Select only scheduled commercial flights and flights with available seats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(154, 1)\n",
      "(36, 1)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "df_airports = scheduledflights[['PASSENGERS','ORIGIN']]\n",
    "df_airports = df_airports.groupby(['ORIGIN']).sum()\n",
    "df_airports = df_airports.sort_values('PASSENGERS', axis=0, ascending=False)\n",
    "df_airports = df_airports[df_airports['PASSENGERS']>350000]\n",
    "print(df_airports.shape)\n",
    "airports = df_airports.index.to_numpy()\n",
    "airports = np.append(airports, 'CMI')\n",
    "airports = np.sort(airports)\n",
    "with open('Top_Airports.csv', 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    for item in airports:\n",
    "        writer.writerow([item])\n",
    "\n",
    "df_carrier = scheduledflights[['PASSENGERS','UNIQUE_CARRIER_NAME']]\n",
    "df_carrier = df_carrier.groupby(['UNIQUE_CARRIER_NAME']).sum()\n",
    "df_carrier = df_carrier.sort_values('PASSENGERS', axis=0, ascending=False)\n",
    "df_carrier = df_carrier[df_carrier['PASSENGERS']>100000]\n",
    "print(df_carrier.shape)\n",
    "carriers = np.sort(df_carrier.index.to_numpy())\n",
    "carriers = np.append('No Selection', carriers)\n",
    "with open('Top_Carriers.csv', 'w', newline='') as outfile:\n",
    "    writer = csv.writer(outfile)\n",
    "    for item in carriers:\n",
    "        writer.writerow([item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduledflights = scheduledflights[(scheduledflights['ORIGIN'].isin(airports)) & (scheduledflights['DEST'].isin(airports)) & (scheduledflights['UNIQUE_CARRIER_NAME'].isin(carriers))]\n",
    "scheduledflights = scheduledflights.reset_index(drop=True)\n",
    "# datasubset = scheduledflights[['UNIQUE_CARRIER_NAME','ORIGIN','DEST','DISTANCE','DEPARTURES_SCHEDULED','MONTH']]\n",
    "datasubset = scheduledflights[['UNIQUE_CARRIER_NAME','ORIGIN','DEST','SEATS','DISTANCE','DEPARTURES_SCHEDULED','MONTH','AIRCRAFT_TYPE']]\n",
    "datasubset.to_csv('All_Flights.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "# ct = ColumnTransformer([('OH-ENCODE', OneHotEncoder(), [0,1,2,3])], remainder = 'passthrough')\n",
    "ct = ColumnTransformer([('OH-ENCODE', OneHotEncoder(), [0,1,2,6,7]),('MinMax', MinMaxScaler(),[3,4,5])], remainder = 'drop')\n",
    "X = ct.fit_transform(datasubset)\n",
    "y = scheduledflights['LOAD_FACTOR']*100\n",
    "pickle.dump(ct, open('OneHotEncoder.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.6360004433250757\n",
      "Mean Error:  13.465582552431353\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "\n",
    "RF_model = RandomForestRegressor(max_depth=25, n_estimators=50)\n",
    "# RF_model = RandomForestRegressor()\n",
    "RF_model.fit(X_train, y_train)\n",
    "score = RF_model.score(X_train, y_train)\n",
    "print(\"R-squared:\", score)\n",
    "y_pred = RF_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Error: \", np.sqrt(mse))\n",
    "pickle.dump(RF_model, open('Load_Factor_RFR_Model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Error = 0.0\n",
      "Max Error = 94.28571428571428\n",
      "Mean Error = 8.884679270474246\n",
      "Cases with less than 5% error = 43.73463857319917 %\n",
      "Cases with less than 10% error = 70.92410142217928 %\n",
      "Cases with less than 20% error = 90.59092914312123 %\n"
     ]
    }
   ],
   "source": [
    "errors = np.abs((y_test.values - y_pred))\n",
    "print('Min Error =',min(errors))\n",
    "print('Max Error =',max(errors))\n",
    "print('Mean Error =',np.mean(errors))\n",
    "print('Cases with less than 5% error =',100*np.sum(errors<5)/len(y_pred),'%')\n",
    "print('Cases with less than 10% error =',100*np.sum(errors<10)/len(y_pred),'%')\n",
    "print('Cases with less than 20% error =',100*np.sum(errors<20)/len(y_pred),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.17593061562723344\n",
      "Mean Error:  14.996130804558849\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# SGD_model = SGDRegressor(alpha = 1e-06, l1_ratio = 0.6, max_iter = 1000, penalty = 'elasticnet')\n",
    "SGD_model = SGDRegressor()\n",
    "SGD_model.fit(X_train, y_train)\n",
    "score = SGD_model.score(X_train, y_train)\n",
    "print(\"R-squared:\", score)\n",
    "y_pred = SGD_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Error: \", np.sqrt(mse))\n",
    "pickle.dump(SGD_model, open('Load_Factor_SGDR_Model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Error = 7.696352187736011e-05\n",
      "Max Error = 91.00349350965492\n",
      "Mean Error = 10.071205027992367\n",
      "Cases with less than 5% error = 36.68871391076115 %\n",
      "Cases with less than 10% error = 64.37375328083989 %\n",
      "Cases with less than 20% error = 89.36272965879265 %\n"
     ]
    }
   ],
   "source": [
    "errors = np.abs((y_test.values - y_pred))\n",
    "print('Min Error =',min(errors))\n",
    "print('Max Error =',max(errors))\n",
    "print('Mean Error =',np.mean(errors))\n",
    "print('Cases with less than 5% error =',100*np.sum(errors<5)/len(y_pred),'%')\n",
    "print('Cases with less than 10% error =',100*np.sum(errors<10)/len(y_pred),'%')\n",
    "print('Cases with less than 20% error =',100*np.sum(errors<20)/len(y_pred),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.17741967187227126\n",
      "Mean Error:  14.987143094066214\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Ridge_model = Ridge(alpha = 10, max_iter = 1000, tol = 0.0000001)\n",
    "Ridge_model = Ridge()\n",
    "Ridge_model.fit(X_train, y_train)\n",
    "score = Ridge_model.score(X_train, y_train)\n",
    "print(\"R-squared:\", score)\n",
    "y_pred = Ridge_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Error: \", np.sqrt(mse))\n",
    "pickle.dump(Ridge_model, open('Load_Factor_RR_Model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Error = 0.00028437904674660786\n",
      "Max Error = 91.14775654117496\n",
      "Mean Error = 10.065608019021314\n",
      "Cases with less than 5% error = 36.888188976377954 %\n",
      "Cases with less than 10% error = 64.449343832021 %\n",
      "Cases with less than 20% error = 89.23464566929134 %\n"
     ]
    }
   ],
   "source": [
    "errors = np.abs((y_test.values - y_pred))\n",
    "print('Min Error =',min(errors))\n",
    "print('Max Error =',max(errors))\n",
    "print('Mean Error =',np.mean(errors))\n",
    "print('Cases with less than 5% error =',100*np.sum(errors<5)/len(y_pred),'%')\n",
    "print('Cases with less than 10% error =',100*np.sum(errors<10)/len(y_pred),'%')\n",
    "print('Cases with less than 20% error =',100*np.sum(errors<20)/len(y_pred),'%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-squared: 0.18394945552141095\n",
      "Mean Error:  15.069733499568917\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "\n",
    "SVR_model = SVR()\n",
    "SVR_model.fit(X_train, y_train)\n",
    "score = SVR_model.score(X_train, y_train)\n",
    "print(\"R-squared:\", score)\n",
    "y_pred = SVR_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Error: \", np.sqrt(mse))\n",
    "pickle.dump(SVR_model, open('Load_Factor_SVR_Model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min Error = 0.0006187632494913942\n",
      "Max Error = 93.82713387456499\n",
      "Mean Error = 8.818762084365845\n",
      "Cases with less than 5% error = 48.86719160104987 %\n",
      "Cases with less than 10% error = 73.96535433070866 %\n",
      "Cases with less than 20% error = 90.91653543307086 %\n"
     ]
    }
   ],
   "source": [
    "errors = np.abs((y_test.values - y_pred))\n",
    "print('Min Error =',min(errors))\n",
    "print('Max Error =',max(errors))\n",
    "print('Mean Error =',np.mean(errors))\n",
    "print('Cases with less than 5% error =',100*np.sum(errors<5)/len(y_pred),'%')\n",
    "print('Cases with less than 10% error =',100*np.sum(errors<10)/len(y_pred),'%')\n",
    "print('Cases with less than 20% error =',100*np.sum(errors<20)/len(y_pred),'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 27 candidates, totalling 81 fits\n",
      "{'max_depth': 100, 'max_features': 'log2', 'n_estimators': 100}\n",
      "Mean Error:  14.188879079400344\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "parameters = {'n_estimators':[50, 75, 100], 'max_depth':[50, 75, 100], 'max_features':[None, 'auto', 'log2']}\n",
    "rfr = RandomForestRegressor()\n",
    "cv = GridSearchCV(rfr, parameters, verbose=4, n_jobs = 5, cv = 3)\n",
    "cv.fit(X_train, y_train)\n",
    "# Fitting 3 folds for each of 75 candidates, totalling 225 fits\n",
    "# [Parallel(n_jobs=5)]: Using backend LokyBackend with 5 concurrent workers.\n",
    "# [Parallel(n_jobs=5)]: Done  22 tasks      | elapsed: 54.2min\n",
    "# [Parallel(n_jobs=5)]: Done 118 tasks      | elapsed: 161.1min\n",
    "# [Parallel(n_jobs=5)]: Done 225 out of 225 | elapsed: 382.4min finished\n",
    "\n",
    "\n",
    "print(cv.best_params_)\n",
    "#{'max_depth': None, 'max_features': 'log2', 'n_estimators': 200}\n",
    "\n",
    "import csv\n",
    "with open('results_cv_RFregressor2019.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=cv.cv_results_.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(cv.cv_results_)\n",
    "\n",
    "y_pred = cv.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Error: \", np.sqrt(mse))\n",
    "# MSE:  144.03809115310145"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Regressor GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 120 candidates, totalling 360 fits\n",
      "{'alpha': 1e-06, 'l1_ratio': 0.6, 'max_iter': 1000, 'penalty': 'elasticnet'}\n",
      "MSE:  235.61281943419226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "parameters = {'penalty':['elasticnet'], \n",
    "              'alpha':[0.000001, 0.000005, 0.00001, 0.00005, 0.0001], \n",
    "              'l1_ratio':[0, 0.2, 0.4, 0.6, 0.8, 1], \n",
    "              'max_iter':[1000,2500,5000,10000]}\n",
    "sgdr = SGDRegressor()\n",
    "best_sgdr = GridSearchCV(sgdr, parameters, verbose=4, n_jobs = 5, cv = 3)\n",
    "best_sgdr.fit(X_train, y_train)\n",
    "\n",
    "print(best_sgdr.best_params_)\n",
    "\n",
    "import csv\n",
    "with open('results_cv_SGDregressor.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=best_sgdr.cv_results_.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(best_sgdr.cv_results_)\n",
    "\n",
    "y_pred = best_sgdr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Error: \", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 80 candidates, totalling 240 fits\n",
      "{'alpha': 10, 'max_iter': 1000, 'tol': 0.0005}\n",
      "MSE:  235.4622751029074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "parameters = {'alpha':[0.1, 0.5, 1, 5, 10], \n",
    "              'max_iter':[1000,2500,5000,10000],\n",
    "              'tol':[0.00005, 0.0005, 0.0001, 0.005]}\n",
    "rr = Ridge()\n",
    "best_rr = GridSearchCV(rr, parameters, verbose=4, n_jobs = 5, cv = 3)\n",
    "best_rr.fit(X_train, y_train)\n",
    "\n",
    "print(best_rr.best_params_)\n",
    "\n",
    "import csv\n",
    "with open('results_cv_Ridgeregressor.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=best_rr.cv_results_.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(best_rr.cv_results_)\n",
    "\n",
    "y_pred = best_rr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Error: \", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVR Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n",
      "{'C': 33, 'degree': 3, 'gamma': 'scale', 'kernel': 'rbf'}\n",
      "Mean Error:  15.344780060427956\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "parameters = {'kernel':['poly','rbf','sigmoid'],\n",
    "              'degree':[3, 5, 7, 9], \n",
    "              'gamma':['scale','auto'],\n",
    "              'C':[1, 3, 10, 33]}\n",
    "svr = SVR()\n",
    "best_svr = GridSearchCV(svr, parameters, verbose=4, n_jobs = 5, cv = 3)\n",
    "best_svr.fit(X_train, y_train)\n",
    "\n",
    "print(best_svr.best_params_)\n",
    "\n",
    "import csv\n",
    "with open('results_cv_SVRregressor.csv', 'w') as csvfile:\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=best_svr.cv_results_.keys())\n",
    "    writer.writeheader()\n",
    "    writer.writerow(best_svr.cv_results_)\n",
    "\n",
    "y_pred = best_rr.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Error: \", np.sqrt(mse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getflightdata(data, origin, destination, carrier, month):\n",
    "    if carrier == 'No Selection':\n",
    "        allcarrierdata = data[(data['ORIGIN']==origin) & (data['DEST']==destination) &\\\n",
    "                       (data['MONTH']==month)]\n",
    "        singlecarrierdata = []\n",
    "        numflights = allcarrierdata.shape[0]\n",
    "        numcarriers = len(allcarrierdata['UNIQUE_CARRIER_NAME'].unique())\n",
    "    else:\n",
    "        allcarrierdata = data[(data['ORIGIN']==origin) & (data['DEST']==destination) &\\\n",
    "                       (data['MONTH']==month)]\n",
    "        singlecarrierdata = allcarrierdata[allcarrierdata['UNIQUE_CARRIER_NAME']==carrier]\n",
    "        numflights = singlecarrierdata.shape[0]\n",
    "        numcarriers = 1\n",
    "    return [allcarrierdata, singlecarrierdata, numflights, numcarriers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
